---
title             : "Through the eyes of the teacher"
shorttitle        : "Visual attention in teaching and learning processes"

author: 
  - name          : "Mandy Klatt"
    affiliation   : "1"
    corresponding : yes    # Define only one corresponding author
    address       : "Egelstraße 2a 04103 Leipzig"
    email         : "mandy.klatt@uni-leipzig.de"
  - name          : "Dr. Gregor Kachel"
    affiliation   : "1, 2"
  - name          : "Dr. Christin Lotz"
    affiliation   : "1"
  - name          : "Prof. Dr. Anne Deiglmayr"
    affiliation   : "1"
  
affiliation:
  - id            : "1"
    institution   : "University of Leipzig"
  - id            : "2"
    institution   : "MPI for Evolutionary Anthropology, Leipzig"

authornote: |
  The study was carried out with the permission of the Ethics Committee of the University of Leipzig.

abstract: |
  This document is a supplement to the paper and shows first graphs findings from the pilot study. 
  
keywords          : "Professional Vision, Expert-Novice-Paradigm, Eye-Tracking"
wordcount         : "1949"

bibliography      : ["r-references.bib"]

floatsintext      : yes
figurelist        : no
tablelist         : no
footnotelist      : no
linenumbers       : yes
mask              : no
draft             : no

documentclass     : "apa6"
classoption       : "man"
output            : word_document #papaja::apa6_pdf
---

```{r setup, include = FALSE}

# if a package is not installed on the current machine, it will install it
if (!require(tidyverse)) install.packages('tidyverse'); library(tidyverse)
if (!require(papaja)) install.packages('papaja'); library(papaja)

r_refs("r-references.bib")

```

```{r analysis-preferences, echo = FALSE}
# Seed for random number generation
set.seed(42)
knitr::opts_chunk$set(cache.extra = knitr::rand_seed)
```

# State of research

Teaching and classroom management are multidimensional settings in which teachers have to respond immediately to events as they develop [@barnes2004significance]. The different interests and abilities of students must be managed in a way that maximizes the active learning time of students and minimizes disruptions whilst teaching. Learning to develop such classroom management skills and to teach effectively is a complicated and complex process [@wolff2017see]. 

During teaching, teachers must be able to select from a variety of visual and acoustic impressions to focus their attention on the essential and to distinguish between relevant and irrelevant events. This ability is called professional vision and is a key component of teacher expertise and successful teaching [@barth2017professionelle]. Eye tracking technology has become a reliable means to study teachers’ visual focus of attention [@pouta2020student; @bogert2016visualperception; @wolff2017see]

Educational research has repeatedly shown that there are differences between experienced and novice teachers in terms of perception and behavioral competencies [@barth2017professionelle; @bogert2016visualperception; @wolff2017see]. For example, experts direct their attention more often and more evenly to all students, whereas novices only direct their attention to some students. The frequency and duration of fixations as eye movement are decisive [@stuermer2017eye]. Mobile eye-tracking technology has also shown that experienced teachers distribute their focus more efficiently to solve tasks [@jarodzka2010eyes]. Furthermore, in contrast to novices, experts are able to focus their attention on the entire class and guide the class while giving feedback to individual students and answering questions [@cortina2015low].



## Research questions

The aim of the pilot study was to investigate whether there are differences in how expert and novice teachers manage scripted classroom disruptions. The disruptions were experimentally varied using a previously written script. Thus, our aim was to find out whether differences in the allocation of attention between expertise groups can be detected in this controlled context.

In order to answer this question, the hypothesis was formulated that teachers with more professional experience not only notice more disruptions but also notice them faster. In the hypothesis, therefore, it is necessary to check what has already been shown in the research literature: In complex teaching situations, experts have a more structured and elaborate professional knowledge than novices in order to perceive and interpret relevant events and to act appropriately [@berliner2001learning; @lachner2016makes].


# Methods
We report how we determined our sample size, all data exclusions (if any), all manipulations, and all measures in the study. <!-- 21-word solution (Simmons, Nelson & Simonsohn, 2012; retrieved from http://ssrn.com/abstract=2160588) -->


## Participants

For the sample recruitment of the subjects (N = 48, experts n = 24, novices n = 24), schools in the city of Leipzig in Saxony were contacted. The institutions as well as the subjects were informed in detail about the aim and intention of the study in advance. Participation in the study was voluntary and only took place after written consent has been given.

```{r demographicspilottable, echo = FALSE}

# prepare data (selected from questionnaire data)
quest.raw.data<-read.table("./data/short_questionnaire_data_2701211.txt", dec=",", sep="\t", header=T) 

# knock out NAs
quest.raw.data <- quest.raw.data  %>% filter(
  personID != "NA")

# create a basic table (tibble) using tidyverse functions
demo.quest.table <- quest.raw.data %>%
  filter(what == "head") %>%
  group_by(group) %>%
  summarise(N=n(),
            "Male" = sum(gender),
            "M age" = round(mean(age), 2),
            "Min age" = min(age),
            "Max age" = max(age),
            "SD age" = round(sd(age), 2),
            "M exp." = round(mean(experience), 2),
            "Min exp." = min(experience),
            "Max exp." = max(experience),
            "SD exp." = round(sd(experience), 2),
           )

# format and insert table in manuscript
apa_table(
  demo.quest.table,
  caption = "Demographic Informationand and Teaching Experience",
  # note = "Write Note here", 
  escape = TRUE, # if TRUE special Latex characters are escaped; if this is turned to F captions cannot be rendered. I don't know why...
  placement = "h", # position of table in page:  exact location (h), at the top (t), bottom (b)
  font_size = "small" # options are tiny, scriptsize, footnotesize, small, normalsize (default), large, Large, LARGE, huge, Huge
  )

```


The selection of the subjects was based on extreme groups, whereby professional experience is the crucial criterion for the selection of experts or novices. Novices were recruited as teachers who have been working in the teaching profession for no more than 3 years, whereas experts were considered to have professional experience of 10 years or more [@messner2000berufliche].

## Data collection

(ref:datacollection-caption) Example for set up during a mini-lesson

```{r datacollection, fig.cap = "(ref:datacollection-caption)", fig.align = "center", echo = FALSE}

knitr::include_graphics("./pictures/datacollection.png", dpi = 108)

# # How to insert images in papaja
# - pictures must be png
# - I created a pictures folder in the main folder, just dumb pictures there
# - you just need to hand the name and path of the picture to the knitr::: function (see above)
# - the dpi argument is for adjusting the size on the page in pixels

# in order to write the caption, you have to use the line just above the code chunk (ref:YOURTEXT-caption) and then just write behind it
# the code chunk then opens and after "r" it needs to provide a name for the code chunk, must be unique in the document
# the fig.cap = then calls the text from the line just above
# fig.align allows to position the pic on page, center ist probablby best
# echo = false ensures that the code chunk does not occur in the pdf
# Now, go and try putting your own images in the paper 
#
# TIP = check line 46 in this manuscript. THis section ets global properties for how graphs are put in the paper. 
#         if floatsintext      : yes  --> pics, graphs, tables will be put where the code chunk is
#         if floatsintext      : no   --> pics, graphs, tables will be put at the end as is required by some journals
# TIP: check the folder "papaja_example&tutorial" that I put in this folder for paper1
# Tip: make a bookmark of this and skim through it: http://frederikaust.com/papaja_man/

```


For this study, scripted mini-lessons with n = 2 experts and n = 6 novices were recorded in the mobile Lab of the Empirical School and Classroom Research at the University of Leipzig. The subjects were divided into groups of four, so the study was conducted on two different sessions. All participants were asked to hold a 10-minute lesson. The duration of each appointment was approximately 2h: per group 10min briefing, 4 x 10min mini-lessons, 10min technical preparation and follow-up and 4x 10min transition points between the lessons and answering the questionnaire.

One person from the group of 4 acted as a teacher, the other three subjects acted as the class. The subjects, who represented the class, were given behavioral instructions in a pre-written script to simulate typical events and disruptions in the classroom (e.g. putting their heads on the table, chatting, looking at their mobile phones, etc.).

The lesson disruptions were displayed as instructions during the lesson for all "students" but not the teacher. In order to avoid learning effects, the disruptions in each lesson were distributed pseudo-randomly over the short teaching phase. In addition, the order of the data collection was taken into account in the analyses and variance caused by order was controlled.

By using mobile eye-trackers, the gaze and behavior of the experts and novices was recorded during the lesson. In addition, the speech and sounds and voices were recorded with an audio recorder installed in the middle of the Lab. Movements, facial expressions and gestures of the subjects were recorded by four cameras from different angles. One camera was installed to film the class from the side. Two more cameras were installed on the blackboard and at the end of the Lab to film the teacher and class from the front and back. Furthermore, the fourth camera was installed in such a way that only facial expressions and gestures of the teacher were recorded, which enables a semi-automated analysis of the movement sequences.

The lessons recorded on video were coded in a post-hoc procedure with a coding software by previously trained raters. The statistical data have been analyzed by using the program RStudio [Link: https://rstudio.com/].


## Measures

### Questionnaire Data
*Describe how we collect questionnaire data (paper or online). Add some basic information about the structure of the questionnaire.* 

The evaluation after each mini-lesson was conducted using paper questionnaires. Time needed to complete the questionnaire was about 5 minutes. 
The scales on the quality of teaching are a validated questionnaire (@helmke2014unterrichtsdiagnostik). Whereas the scales on the teacher's presence behaviour were derived from the research literature (@brophy1986classroom; @kiel2013trainingsbuch; @kounin2006techniken; @marzano2007art; @nolting2012storungen) and were used in the pilot for the first time. 

The following scales were assessed:

(1) Classroom management
(2) Positive climate and motivation
(3) Clarity and structuredness
(4) Activation and support
(5) Presence: posture/gaze
(6) Presence: voice
(7) Presence: verbal and non-verbal intervention
(8) Natural behaviour

The table below provides an overview over the mean, min, max and standard deviation of all scales.

```{r table all scales, echo = FALSE}

# prepare data (selected from questionnaire data)
quest.raw.data<-read.table("./data/short_questionnaire_data_2701211.txt", dec=",", sep="\t", header=T) 

# knock out NAs
quest.raw.data <- quest.raw.data  %>% filter(
  personID != "NA")

# to compare both sessions filter only Identical items
quest.raw.data <- quest.raw.data  %>% filter(str_detect(shortID, 
                                                 paste(c("km_mitbekommen",
                                                         "km_klar",
                                                         "km_ungestört",
                                                         "km_aktiv",
                                                         "lkm_freundlich",
                                                         "lkm_interesse",
                                                         "lkm_kritik",
                                                         "lkm_ausreden",
                                                         "lkm_überlegen",
                                                         "lkm_rückmeldungen",
                                                         "ks_funktion",
                                                         "ks_verständlich",
                                                         "ks_sichtbar",
                                                         "af_beiträge",
                                                         "af_wechselseitig",
                                                         "af_nachdenken",
                                                         "phb_stand",
                                                         "phb_augen",
                                                         "phb_blick",
                                                         "phb_vorsichgeht",
                                                         "phb_raum",
                                                         "phb_gestik",
                                                         "phb_alleangesehen",
                                                         "ps_deutlich",
                                                         "ps_klar",
                                                         "ps_impulse",
                                                         "pi_nonverbal",
                                                         "pi_zubewegen",
                                                         "pi_direkt",
                                                         "m_natürlich",
                                                         "m_fiktiv",
                                                         "m_verhalten"),
                                                    collapse = '|')))

# value sometimes contained text before filtering, we have to convert the numbers to numeric
quest.raw.data <- quest.raw.data %>%  mutate(value = as.numeric(value))

# create a basic table (tibble) using tidyverse functions 
scale.quest.table <- quest.raw.data %>% # select data
  group_by(group, scale) %>%
  summarise("M scale" = round(mean(value), 2),
            "min scale" = min(value),
            "max sclae" = max(value),
            "SD scale" = round(sd(value), 2))

# remove duplicate labels from the table column "group" to create the APA-style omission of categorical variables
# this is easier than the stub_indent option in apa_table
scale.quest.table$group[duplicated(scale.quest.table$group)] <- ""

# format and insert table in manuscript
apa_table(
  scale.quest.table,
  caption = "Mean values of all scales",
  # note = "Write Note here", 
  escape = TRUE, # if TRUE special Latex characters are escaped; if this is turned to F captions cannot be rendered. I don't know why...
  placement = "h", # position of table in page:  exact location (h), at the top (t), bottom (b)
  font_size = "small" # options are tiny, scriptsize, footnotesize, small, normalsize (default), large, Large, LARGE, huge, Huge
)

```

\newpage
The individual items of a scale were further represented in graphs.

(1) Classroom management
```{r classroom management line plots,  fig.width=6, fig.height = 6, message = FALSE, warning = FALSE , echo=FALSE}

### classroom management 

# filter by parameter variable, i.e. create a subset for classroom management
cm.data <- quest.raw.data %>% filter(scale == "Classroom management")

# value sometimes contained text before filtering, we have to convert the numbers to numeric
cm.data <- cm.data %>%  mutate(value = as.numeric(as.character(factor(value))))

# to create error bars, we need to summarize the data in a separate data frame
cm.plot.sd <- cm.data %>%
  group_by(group, item.wordings, .drop=TRUE) %>%
  summarise(perspective, mean = mean(value), n = n(), sd = sd(value), se = sd/sqrt(n), )

# long plot
cm.plot<-ggplot(data = cm.plot.sd,
                  aes(x = item.wordings, y = mean,
                      group = group, colour = group)) +
  geom_line()+
  geom_pointrange(position = position_dodge(0.1), aes(ymin = mean-se, ymax = mean+se))+
  theme_light() +
  ylim(1,4)+
  facet_grid(~perspective)+
  labs(data = cm.data, y = "value", x = NULL,
       title="Classroom management" ,
       subtitle=NULL)+
  theme(legend.position="bottom",
        panel.spacing.x = ,
        plot.title = element_text(hjust = 0.5),
        axis.text.y = element_text(size = 6))+
  scale_x_discrete(labels = function(x) str_wrap(x, width = 55))+
  coord_flip()
cm.plot

```
\newpage
(2) Positive climate and motivation
```{r Positive climate and motivation line plots,  fig.width=6, fig.height = 7, message = FALSE, warning = FALSE , echo=FALSE}

### Positive climate and motivation 

# filter by parameter variable, i.e. create a subset for Positive climate and motivation
pcm.data <- quest.raw.data %>% filter(scale == "Positive climate and motivation")

# value sometimes contained text before filtering, we have to convert the numbers to numeric
pcm.data <- pcm.data %>%  mutate(value = as.numeric(as.character(factor(value))))

# to create error bars, we need to summarize the data in a separate data frame
pcm.plot.sd <- pcm.data %>%
  group_by(group, item.wordings, .drop=TRUE) %>%
  summarise(perspective, mean = mean(value), n = n(), sd = sd(value), se = sd/sqrt(n), )

# long plot
pcm.plot<-ggplot(data = pcm.plot.sd,
                  aes(x = item.wordings, y = mean,
                      group = group, colour = group)) +
  geom_line()+
  geom_pointrange(position = position_dodge(0.1), aes(ymin = mean-se, ymax = mean+se))+
  theme_light() +
  ylim(1,4)+
  facet_grid(~perspective)+
  labs(data = pcm.data, y = "value", x = NULL,
       title="Positive climate and motivation" ,
       subtitle=NULL)+
  theme(legend.position="bottom",
        panel.spacing.x = ,
        plot.title = element_text(hjust = 0.5),
        axis.text.y = element_text(size = 5))+
  scale_x_discrete(labels = function(x) str_wrap(x, width = 55))+
  coord_flip()
pcm.plot

```
\newpage
(3) Clarity and structuredness
```{r Clarity and structuredness line plots,  fig.width=6, fig.height = 6, message = FALSE, warning = FALSE , echo=FALSE}

### Clarity and structuredness 

# filter by parameter variable, i.e. create a subset for Clarity and structuredness
cs.data <- quest.raw.data %>% filter(scale == "Clarity and structuredness")

# value sometimes contained text before filtering, we have to convert the numbers to numeric
cs.data <- cs.data %>%  mutate(value = as.numeric(as.character(factor(value))))

# to create error bars, we need to summarize the data in a separate data frame
cs.plot.sd <- cs.data %>%
  group_by(group, item.wordings, .drop=TRUE) %>%
  summarise(perspective, mean = mean(value), n = n(), sd = sd(value), se = sd/sqrt(n), )

# long plot
cs.plot<-ggplot(data = cs.plot.sd,
                  aes(x = item.wordings, y = mean,
                      group = group, colour = group)) +
  geom_line()+
  geom_pointrange(position = position_dodge(0.1), aes(ymin = mean-se, ymax = mean+se))+
  theme_light() +
  ylim(1,4)+
  facet_grid(~perspective)+
  labs(data = cs.data, y = "value", x = NULL,
       title="Clarity and structuredness" ,
       subtitle=NULL)+
  theme(legend.position="bottom",
        panel.spacing.x = ,
        plot.title = element_text(hjust = 0.5),
        axis.text.y = element_text(size = 6))+
  scale_x_discrete(labels = function(x) str_wrap(x, width = 55))+
  coord_flip()
cs.plot

```
\newpage
(4) Activation and support
```{r Activation and support line plots,  fig.width=6, fig.height = 6, message = FALSE, warning = FALSE , echo=FALSE}

### Activation and support 

# filter by parameter variable, i.e. create a subset for Activation and support
as.data <- quest.raw.data %>% filter(scale == "Activation and support")

# value sometimes contained text before filtering, we have to convert the numbers to numeric
as.data <- as.data %>%  mutate(value = as.numeric(as.character(factor(value))))

# to create error bars, we need to summarize the data in a separate data frame
as.plot.sd <- as.data %>%
  group_by(group, item.wordings, .drop=TRUE) %>%
  summarise(perspective, mean = mean(value), n = n(), sd = sd(value), se = sd/sqrt(n), )

# long plot
as.plot<-ggplot(data = as.plot.sd,
                  aes(x = item.wordings, y = mean,
                      group = group, colour = group)) +
  geom_line()+
  geom_pointrange(position = position_dodge(0.1), aes(ymin = mean-se, ymax = mean+se))+
  theme_light() +
  ylim(1,4)+
  facet_grid(~perspective)+
  labs(data = as.data, y = "value", x = NULL,
       title="Activation and support" ,
       subtitle=NULL)+
  theme(legend.position="bottom",
        panel.spacing.x = ,
        plot.title = element_text(hjust = 0.5),
        axis.text.y = element_text(size = 6))+
  scale_x_discrete(labels = function(x) str_wrap(x, width = 55))+
  coord_flip()
as.plot

```
\newpage

(ref:presenceposturegaze-caption) (5) Presence: posture/gaze 

```{r presenceposturegaze, fig.cap = "(ref:presenceposturegaze-caption)", fig.align = "center", echo = FALSE}

knitr::include_graphics("./pictures/presenceposturegaze.png", dpi = 108)

# ```{r Presence: posture/gaze line plots,  fig.width=6, fig.height = 6, message = FALSE, warning = FALSE , echo=FALSE}

# ### Presence: posture/gaze 
# 
# # filter by parameter variable, i.e. create a subset for Presence: posture/gaze
# ppg.data <- quest.raw.data %>% filter(scale == "Presence: posture/gaze")
# 
# # value sometimes contained text before filtering, we have to convert the numbers to numeric
# ppg.data <- ppg.data %>%  mutate(value = as.numeric(as.character(factor(value))))
# 
# # to create error bars, we need to summarize the data in a separate data frame
# ppg.plot.sd <- ppg.data %>%
#   group_by(group, item.wordings, .drop=TRUE) %>%
#   summarise(perspective, mean = mean(value), n = n(), sd = sd(value), se = sd/sqrt(n), )
# 
# # long plot
# ppg.plot<-ggplot(data = ppg.plot.sd,
#                   aes(x = item.wordings, y = mean,
#                       group = group, colour = group)) +
#   geom_line()+
#   geom_pointrange(position = position_dodge(0.1), aes(ymin = mean-se, ymax = mean+se))+
#   theme_light() +
#   ylim(1,4)+
#   facet_grid(~perspective)+
#   labs(data = ppg.data, y = "value", x = NULL,
#        title="Presence: posture/gaze" ,
#        subtitle=NULL)+
#   theme(legend.position="bottom",
#         panel.spacing.x = ,
#         plot.title = element_text(hjust = 0.5),
#         axis.text.y = element_text(size = 6))+
#   scale_x_discrete(labels = function(x) str_wrap(x, width = 55))+
#   coord_flip()
# ppg.plot

```

\newpage

(ref:presencevoice-caption) (6) Presence: voice

```{r presencevoice, fig.cap = "(ref:presencevoice-caption)", fig.align = "center", echo = FALSE}

knitr::include_graphics("./pictures/presencevoice.png", dpi = 108)

# ```{r Presence: voice line plots,  fig.width=6, fig.height = 6, message = FALSE, warning = FALSE , echo=FALSE}
# 
# ### Presence: voice 
# # filter by parameter variable, i.e. create a subset for Presence: voice
# pv.data <- quest.raw.data %>% filter(scale == "Presence: voice")
# 
# # value sometimes contained text before filtering, we have to convert the numbers to numeric
# pv.data <- pv.data %>%  mutate(value = as.numeric(as.character(factor(value))))
# 
# # to create error bars, we need to summarize the data in a separate data frame
# pv.plot.sd <- pv.data %>%
#   group_by(group, item.wordings, .drop=TRUE) %>%
#   summarise(perspective, mean = mean(value), n = n(), sd = sd(value), se = sd/sqrt(n), )
# 
# # long plot
# pv.plot<-ggplot(data = pv.plot.sd,
#                   aes(x = item.wordings, y = mean,
#                       group = group, colour = group)) +
#   geom_line()+
#   geom_pointrange(position = position_dodge(0.1), aes(ymin = mean-se, ymax = mean+se))+
#   theme_light() +
#   ylim(1,4)+
#   facet_grid(~perspective)+
#   labs(data = pv.data, y = "value", x = NULL,
#        title="Presence: voice" ,
#        subtitle=NULL)+
#   theme(legend.position="bottom",
#         panel.spacing.x = ,
#         plot.title = element_text(hjust = 0.5),
#         axis.text.y = element_text(size = 6))+
#   scale_x_discrete(labels = function(x) str_wrap(x, width = 55))+
#   coord_flip()
# pv.plot

```
\newpage

(ref:presenceverbalnonverbalintervention-caption) (7) Presence: verbal and non-verbal intervention
```{r presenceverbalnonverbalintervention, fig.cap = "(ref:presenceverbalnonverbalintervention-caption)", fig.align = "center", echo = FALSE}

knitr::include_graphics("./pictures/presenceverbalnonverbalintervention.png", dpi = 108)

# ```{r Presence: verbal and non-verbal intervention line plots,  fig.width=6, fig.height = 6, message = FALSE, warning = FALSE , echo=FALSE}
# 
# ### Presence: verbal and non-verbal intervention 
# # filter by parameter variable, i.e. create a subset for Presence: verbal and non-verbal intervention
# pvni.data <- quest.raw.data %>% filter(scale == "Presence: verbal and non-verbal intervention")
# 
# # value sometimes contained text before filtering, we have to convert the numbers to numeric
# pvni.data <- pvni.data %>%  mutate(value = as.numeric(as.character(factor(value))))
# 
# # to create error bars, we need to summarize the data in a separate data frame
# pvni.plot.sd <- pvni.data %>%
#   group_by(group, item.wordings, .drop=TRUE) %>%
#   summarise(perspective, mean = mean(value), n = n(), sd = sd(value), se = sd/sqrt(n), )
# 
# # long plot
# pvni.plot<-ggplot(data = pvni.plot.sd,
#                   aes(x = item.wordings, y = mean,
#                       group = group, colour = group)) +
#   geom_line()+
#   geom_pointrange(position = position_dodge(0.1), aes(ymin = mean-se, ymax = mean+se))+
#   theme_light() +
#   ylim(1,4)+
#   facet_grid(~perspective)+
#   labs(data = pvni.data, y = "value", x = NULL,
#        title="Presence: verbal and non-verbal intervention" ,
#        subtitle=NULL)+
#   theme(legend.position="bottom",
#         panel.spacing.x = ,
#         plot.title = element_text(hjust = 0.5),
#         axis.text.y = element_text(size = 6))+
#   scale_x_discrete(labels = function(x) str_wrap(x, width = 55))+
#   coord_flip()
# pvni.plot

```
\newpage
(8) Natural behaviour
```{r Natural behaviour line plots,  fig.width=6, fig.height = 6, message = FALSE, warning = FALSE , echo=FALSE}

### Natural behaviour 
# filter by parameter variable, i.e. create a subset for Natural behaviour
nb.data <- quest.raw.data %>% filter(scale == "Natural behaviour")

# value sometimes contained text before filtering, we have to convert the numbers to numeric
nb.data <- nb.data %>%  mutate(value = as.numeric(as.character(factor(value))))

# to create error bars, we need to summarize the data in a separate data frame
nb.plot.sd <- nb.data %>%
  group_by(group, item.wordings, .drop=TRUE) %>%
  summarise(perspective, mean = mean(value), n = n(), sd = sd(value), se = sd/sqrt(n), )

# long plot
nb.plot<-ggplot(data = nb.plot.sd,
                  aes(x = item.wordings, y = mean,
                      group = group, colour = group)) +
  geom_line()+
  geom_pointrange(position = position_dodge(0.1), aes(ymin = mean-se, ymax = mean+se))+
  theme_light() +
  ylim(1,4)+
  facet_grid(~perspective)+
  labs(data = nb.data, y = "value", x = NULL,
       title="Natural behaviour" ,
       subtitle=NULL)+
  theme(legend.position="bottom",
        panel.spacing.x = ,
        plot.title = element_text(hjust = 0.5),
        axis.text.y = element_text(size = 6))+
  scale_x_discrete(labels = function(x) str_wrap(x, width = 55))+
  coord_flip()
nb.plot

```
\newpage
In addition, we plotted all scales. Graph provides boxplots and individual data for experts and novices.
```{r boxplot scales, fig.width=6, fig.height = 8, echo = FALSE}

quest.plot <- quest.raw.data %>%
  ggplot( aes(x=scale, y=value, fill=scale)) +
    geom_boxplot() +
    scale_x_discrete(guide = guide_axis(angle = 70)) +
    geom_jitter(color="black", size=0.4, alpha=0.9) +
    theme_light() +
    theme(
      legend.position="none",
      plot.title = element_text(size=11)
    ) +
    facet_grid(~group)+
    ggtitle("Boxplot with individual points for all scales") +
    xlab("")
quest.plot

```
\newpage
Furthermore, the duration of the speaking time during the lesson was estimated in the questionnaire by external and self-assessment. The following graph shows the duration of speaking time for experts and novices evaluated by the observer, the students and the teacher.

```{r Duration speaking time, fig.width=5, fig.height = 5, echo = FALSE}
# prepare data (selected from questionnaire data)
quest.raw.data<-read.table("./data/short_questionnaire_data_2701211.txt", dec=",", sep="\t", header=T) 

# knock out NAs
quest.raw.data <- quest.raw.data  %>% filter(
  personID != "NA")

# to compare both sessions filter only Identical items
quest.raw.data <- quest.raw.data  %>% filter(str_detect(scale, 
                                                 paste(c("Duration of speaking time"),
                                                    collapse = '|')))
view(quest.raw.data)

# value sometimes contained text before filtering, we have to convert the numbers to numeric
quest.raw.data <- quest.raw.data %>%  mutate(value = as.numeric(value))

speaking.plot <- quest.raw.data %>%
  ggplot(aes(x='Duration of speaking time', y=value, fill=group)) +
    geom_boxplot() +
    geom_jitter(color="black", size=0.4, alpha=0.9) +
    theme_light() +
    theme(
      legend.position="none",
      plot.title = element_text(size=11),
      axis.text.x = element_text(size = 8)) +
    facet_grid(group~perspective) +
    ggtitle("Boxplot with individual points for Duration of speaking time") +
    xlab("")
speaking.plot

```
\newpage

### Eye-tracking equipment 

A binocular Tobii Pro Glasses 2 eye-tracker consisting of a wearable head unit and a recording unit was used to record the eye movements. The head unit is a measuring device with different sensitive sensors. A high-definition scene camera captures a full HD video and an integrated microphone records the surrounding sounds. Infrared light illuminators support the eye tracking sensors which record the eye orientation. The videos were recorded with a sampling rate of 50 Hz and a video resolution with 1920 x 1080 at 25 frames per second. The scene camera has a field of view of 90 deg. in 16:9 format (82 deg. horizontal and 52 deg. vertical) and has a frame dimension of 179 x 159 x 57mm (width x depth x height). The Tobii Pro Glasses Controller software was used to record and calibrate the eye movements.

The Tobii Pro Glasses 2 software allows for non-screen based recordings of a participants’ attention while moving in real-world settings. The recordings of the glasses contain both HD-video from the subjects' perspective as well as the respective gaze data mapped onto the video. In order to map multiple recordings to AOIs, it is necessary to import the eye-tracking recordings into the Tobii Pro Analyzer software. Also, it is necessary to create a reference image of the scene in which one wishes to plot the gaze data (i.e. snapshot). Once the snapshot is imported, the gaze recordings of multiple recordings can be mapped to the reference image and analyzed in aggregated form. Tobii Pro does not allow to do AOI based analyses within Pro Lab. Also, the dependency on snapshot reference images makes this approach impractical when working in different settings, i.e. different classrooms with various participants. Finally, mapping gaze to people or any moving objects complicated the analyses further.


## Data analysis
We used `r cite_r("r-references.bib")` for all our analyses.

Lorem ipsum dolor sit amet, consetetur sadipscing elitr, sed diam nonumy eirmod tempor invidunt ut labore et dolore magna aliquyam erat, sed diam voluptua. At vero eos et accusam et justo duo dolores et ea rebum. Stet clita kasd gubergren, no sea takimata sanctus est Lorem ipsum dolor sit amet. Lorem ipsum dolor sit amet, consetetur sadipscing elitr, sed diam nonumy eirmod tempor invidunt ut labore et dolore magna aliquyam erat, sed diam voluptua. At vero eos et accusam et justo duo dolores et ea rebum. Stet clita kasd gubergren, no sea takimata sanctus est Lorem ipsum dolor sit amet.

# Results

### Questionnaire Data
*Start entering descriptives and plots here* Lorem ipsum dolor sit amet, consetetur sadipscing elitr, sed diam nonumy eirmod tempor invidunt ut labore et dolore magna aliquyam erat, sed diam voluptua. At vero eos et accusam et justo duo dolores et ea rebum. Stet clita kasd gubergren, no sea takimata sanctus est Lorem ipsum dolor sit amet. Lorem ipsum dolor sit amet, consetetur sadipscing elitr, sed diam nonumy eirmod tempor invidunt ut labore et dolore magna aliquyam erat, sed diam voluptua. At vero eos et accusam et justo duo dolores et ea rebum. Stet clita kasd gubergren, no sea takimata sanctus est Lorem ipsum dolor sit amet.

Lorem ipsum dolor sit amet, consetetur sadipscing elitr, sed diam nonumy eirmod tempor invidunt ut labore et dolore magna aliquyam erat, sed diam voluptua. At vero eos et accusam et justo duo dolores et ea rebum. Stet clita kasd gubergren, no sea takimata sanctus est Lorem ipsum dolor sit amet. Lorem ipsum dolor sit amet, consetetur sadipscing elitr, sed diam nonumy eirmod tempor invidunt ut labore et dolore magna aliquyam erat, sed diam voluptua. At vero eos et accusam et justo duo dolores et ea rebum. Stet clita kasd gubergren, no sea takimata sanctus est Lorem ipsum dolor sit amet.


# Discussion

Lorem ipsum dolor sit amet, consetetur sadipscing elitr, sed diam nonumy eirmod tempor invidunt ut labore et dolore magna aliquyam erat, sed diam voluptua. At vero eos et accusam et justo duo dolores et ea rebum. Stet clita kasd gubergren, no sea takimata sanctus est Lorem ipsum dolor sit amet. Lorem ipsum dolor sit amet, consetetur sadipscing elitr, sed diam nonumy eirmod tempor invidunt ut labore et dolore magna aliquyam erat, sed diam voluptua. At vero eos et accusam et justo duo dolores et ea rebum. Stet clita kasd gubergren, no sea takimata sanctus est Lorem ipsum dolor sit amet.

\newpage

# References

\begingroup
\setlength{\parindent}{-0.5in}
\setlength{\leftskip}{0.5in}

<div id="refs" custom-style="Bibliography"></div>
\endgroup
